{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "982f1db3-8405-4258-a4cf-e263de77ae14",
   "metadata": {},
   "source": [
    "# Installing Dependencies for LangChain with Groq LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1ee0228-dbeb-4a13-9309-8bf9f1d6603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all necessary dependencies for LangChain with Groq integration\n",
    "#!pip install langchain langchain-groq langchain-core python-dotenv\n",
    "#!pip install sentence-transformers\n",
    "#!pip install pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d28f2c1-5fb1-40f4-b21e-4848ad031e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mbkhn\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from pinecone import Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fada5cb8-de4a-4a37-8532-350f5aae6086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ee5485d-394d-4c79-a569-10b690b0649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the LLM -- LLM Wrappers\n",
    "llm = ChatGroq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    "    model_name=\"llama3-70b-8192\"  # You can also use \"mixtral-8x7b-32768\" or other available models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "763b85bf-cc9d-46dd-9632-e14bfa015cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Prompt_Template -- Prompts\n",
    "template = \"\"\"\n",
    "You are an expert data scientist with an expertise in building deep learning models.\n",
    "Explain the concept of {concept} in a couple of lines\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variable=[\"concept\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cbae79e-9b6e-4a28-a4d8-274061ce529f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['concept'], input_types={}, partial_variables={}, template='\\nYou are an expert data scientist with an expertise in building deep learning models.\\nExplain the concept of {concept} in a couple of lines\\n')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fdfba8c-2bad-417e-afd5-aba65ccec494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Regularization is a technique used in deep learning to prevent overfitting by adding a penalty term to the loss function, which discourages the model from fitting the training data too closely, and instead encourages it to generalize better to new, unseen data. This is achieved by adding a term that measures the complexity of the model, such as the magnitude of the weights, to the loss function, which helps to prevent the model from becoming too complex and overfitting to the training data.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 37, 'total_tokens': 135, 'completion_time': 0.354125012, 'prompt_time': 0.000888584, 'queue_time': 0.052829845, 'total_time': 0.355013596}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--637a6385-fe19-47c3-9f76-f785ffc26bfd-0', usage_metadata={'input_tokens': 37, 'output_tokens': 98, 'total_tokens': 135})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke([HumanMessage(content=prompt.format(concept=\"regularization\"))])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc7071c0-733c-42be-8f54-5ac1b35c2059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='An autoencoder is a type of neural network that learns to compress and reconstruct its input data by mapping it to a lower-dimensional representation, called the bottleneck or latent representation, and then reconstructing the original input from this representation. In essence, an autoencoder is trained to copy its input to its output, thereby learning to identify the most important features of the data and ignore the noise or irrelevant information.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 81, 'prompt_tokens': 38, 'total_tokens': 119, 'completion_time': 0.293842422, 'prompt_time': 0.000899734, 'queue_time': 0.054964585, 'total_time': 0.294742156}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_dd4ae1c591', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--c07a8f3c-573a-4934-a1d7-5231e0812d56-0', usage_metadata={'input_tokens': 38, 'output_tokens': 81, 'total_tokens': 119})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke([HumanMessage(content=prompt.format(concept=\"autoencoder\"))])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca79f781-95d6-4dfb-9406-a294c4f458f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSetting up Chains -- \\nA chain takes a language model and a prompt template and combines them into a interface \\nthat thakes an 'input' from the user and 'outputs' an answer from an LLM\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Setting up Chains -- \n",
    "A chain takes a language model and a prompt template and combines them into a interface \n",
    "that thakes an 'input' from the user and 'outputs' an answer from an LLM\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94007f8c-bfc7-45fa-97db-99aa953dd171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mbkhn\\AppData\\Local\\Temp\\ipykernel_21580\\4138082450.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm = llm, prompt = prompt)\n",
      "C:\\Users\\mbkhn\\AppData\\Local\\Temp\\ipykernel_21580\\4138082450.py:4: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(chain.run(\"autoencoder\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An autoencoder is a type of neural network that learns to compress and reconstruct its input data by mapping it to a lower-dimensional latent representation, called the bottleneck or encoding, and then reconstructing the original input from this representation. This process allows the autoencoder to learn a compact and efficient representation of the data, which can be useful for tasks such as dimensionality reduction, anomaly detection, and generative modeling.\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm = llm, prompt = prompt)\n",
    "\n",
    "# Run the cahin only specifying the input variable\n",
    "print(chain.run(\"autoencoder\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3971f52f-4f4e-4985-9493-58d95db349e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = PromptTemplate(\n",
    "    input_variables = [\"ml_concept\"],\n",
    "    template = \"Turn the concept description of{ml_concepts} and explain it to me like I'm five\",\n",
    ")\n",
    "\n",
    "chain_two = LLMChain(llm = llm, prompt = second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1db01a1a-3a8c-4dd3-b82d-249fe445486d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mAn autoencoder is a type of neural network that learns to compress and reconstruct its input data by mapping it to a lower-dimensional representation, called the bottleneck or latent representation, and then reconstructing the original data from this representation. This process allows the autoencoder to learn a compact and meaningful representation of the data, which can be used for tasks such as dimensionality reduction, anomaly detection, and generative modeling.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mOh boy, are you ready for a fun explanation?\n",
      "\n",
      "Imagine you have a big box full of toys, and inside that box, you have lots of different toys like blocks, dolls, cars, and puzzles. Now, imagine you want to send this box of toys to your friend who lives far, far away.\n",
      "\n",
      "But, there's a problem! The box is too big and heavy to send, and it would cost a lot of money to mail it. So, you need to find a way to make the box smaller and lighter, without losing any of the toys inside.\n",
      "\n",
      "That's kind of like what an autoencoder does. An autoencoder is a special kind of computer program that helps us make a smaller version of our big box of toys (or data). It does this by finding a way to squeeze all the toys into a tiny little box, called the \"bottleneck\" or \"latent representation\". This tiny box is like a special summary of all the toys in the big box.\n",
      "\n",
      "But here's the magic part! The autoencoder can then take this tiny box and use it to rebuild the original big box of toys, with all the same blocks, dolls, cars, and puzzles inside. It's like the autoencoder has a special set of instructions that says, \"Hey, I know what all these toys look like, so I can use this tiny box to make a brand new big box that's just like the original!\"\n",
      "\n",
      "So, why is this helpful? Well, having a smaller version of our data (the tiny box) makes it easier to work with and understand. We can use this smaller version to do things like:\n",
      "\n",
      "* Find weird or broken toys (anomaly detection)\n",
      "* Make new toys that are similar to the ones we already have (generative modeling)\n",
      "* Understand what makes all the toys similar or different (dimensionality reduction)\n",
      "\n",
      "And that's autoencoders in a nutshell!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Oh boy, are you ready for a fun explanation?\n",
      "\n",
      "Imagine you have a big box full of toys, and inside that box, you have lots of different toys like blocks, dolls, cars, and puzzles. Now, imagine you want to send this box of toys to your friend who lives far, far away.\n",
      "\n",
      "But, there's a problem! The box is too big and heavy to send, and it would cost a lot of money to mail it. So, you need to find a way to make the box smaller and lighter, without losing any of the toys inside.\n",
      "\n",
      "That's kind of like what an autoencoder does. An autoencoder is a special kind of computer program that helps us make a smaller version of our big box of toys (or data). It does this by finding a way to squeeze all the toys into a tiny little box, called the \"bottleneck\" or \"latent representation\". This tiny box is like a special summary of all the toys in the big box.\n",
      "\n",
      "But here's the magic part! The autoencoder can then take this tiny box and use it to rebuild the original big box of toys, with all the same blocks, dolls, cars, and puzzles inside. It's like the autoencoder has a special set of instructions that says, \"Hey, I know what all these toys look like, so I can use this tiny box to make a brand new big box that's just like the original!\"\n",
      "\n",
      "So, why is this helpful? Well, having a smaller version of our data (the tiny box) makes it easier to work with and understand. We can use this smaller version to do things like:\n",
      "\n",
      "* Find weird or broken toys (anomaly detection)\n",
      "* Make new toys that are similar to the ones we already have (generative modeling)\n",
      "* Understand what makes all the toys similar or different (dimensionality reduction)\n",
      "\n",
      "And that's autoencoders in a nutshell!\n"
     ]
    }
   ],
   "source": [
    "overall_chain = SimpleSequentialChain(chains = [chain, chain_two], verbose=True)\n",
    "\n",
    "# Run the chain specifying only the input variable for the first chain\n",
    "explanation= overall_chain.run(\"autoencoder\")\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9a519146-7e8e-40c8-bb73-38f22cf18468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Embeddings and VectorStores\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Embeddings and VectorStores\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ebf8cb43-d3c5-4201-a29f-b332bfe7f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap = 0,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([explanation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8ebd882e-f306-49ba-bd48-5e75d19dd927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh boy, are you ready for a fun explanation?'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f810eb29-cfb1-4b97-bfe4-015932404656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding model -- from HuggingFace\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d20ba317-7085-4148-893a-166ca7677d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_embedding = embedding(texts[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b2422f47-a720-4b5c-baa6-eac50781e829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d22f0ffc-8cf5-48ca-8432-7c384ab11306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dimension = query_embedding.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2dde9fec-08ee-4f00-bd14-ba4a4749e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# ✅ Initialize Pinecone client\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pc = Pinecone(api_key=api_key)  # Replace with your actual Pinecone API key\n",
    "\n",
    "# Delete existing index\n",
    "pc.delete_index(\"langchain-quickstart\")\n",
    "\n",
    "# ✅ Create index\n",
    "pc.create_index(\n",
    "    name=\"langchain-quickstart\",\n",
    "    dimension=384,\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fb1cf699-340e-4aaf-a395-942c7145eae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Pinecone as LangPinecone\n",
    "\n",
    "index_name = \"langchain-quickstart\"\n",
    "search = LangPinecone.from_documents(\n",
    "    documents=texts,\n",
    "    embedding=embedding,\n",
    "    index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b9a09551-3f0f-4a0a-8d7e-dc28646bb1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And that's autoencoders in a nutshell!\n",
      "That's kind of like what an autoencoder does. An autoencoder is a special kind of computer program\n",
      "autoencoder has a special set of instructions that says, \"Hey, I know what all these toys look\n",
      "But here's the magic part! The autoencoder can then take this tiny box and use it to rebuild the\n"
     ]
    }
   ],
   "source": [
    "query = \"What is an autoencoder?\"\n",
    "results = search.similarity_search(query, k=4)\n",
    "for r in results:\n",
    "    print(r.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b59d0d5f-8ff1-4243-b346-a926aa2b87e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content=\"And that's autoencoders in a nutshell!\"),\n",
       " Document(metadata={}, page_content=\"That's kind of like what an autoencoder does. An autoencoder is a special kind of computer program\"),\n",
       " Document(metadata={}, page_content='autoencoder has a special set of instructions that says, \"Hey, I know what all these toys look'),\n",
       " Document(metadata={}, page_content=\"But here's the magic part! The autoencoder can then take this tiny box and use it to rebuild the\")]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "abe79b96-1457-4411-ad55-720584e525ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Oh boy, are you ready for a fun explanation?' metadata={'text': 'Oh boy, are you ready for a fun explanation?'}\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "910fd362-5954-45db-afd9-ac655a9f62c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Agents\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Agents\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "09f3bb56-c3c7-4379-9423-33e97382fa8d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: langchain 0.3.26\n",
      "Uninstalling langchain-0.3.26:\n",
      "  Successfully uninstalled langchain-0.3.26\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.26-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in c:\\users\\mbkhn\\anaconda3\\lib\\site-packages (from langchain) (0.3.68)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\mbkhn\\anaconda3\\lib\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\mbkhn\\anaconda3\\lib\\site-packages (from langchain) (0.3.45)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\mbkhn\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\mbkhn\\anaconda3\\lib\\site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\mbkhn\\anaconda3\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\mbkhn\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\mbkhn\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (8.2.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\mbkhn\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\mbkhn\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\mbkhn\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\mbkhn\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\mbkhn\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\mbkhn\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\mbkhn\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\mbkhn\\appdata\\roaming\\python\\python312\\site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\mbkhn\\appdata\\roaming\\python\\python312\\site-packages (from langsmith>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\mbkhn\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\mbkhn\\anaconda3\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\mbkhn\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.8.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\mbkhn\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\mbkhn\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\mbkhn\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\mbkhn\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\mbkhn\\appdata\\roaming\\python\\python312\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
      "Downloading langchain-0.3.26-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 6.8 MB/s eta 0:00:00\n",
      "Installing collected packages: langchain\n",
      "Successfully installed langchain-0.3.26\n"
     ]
    }
   ],
   "source": [
    "#pip install langchain-experimental\n",
    "#!pip uninstall langchain -y\n",
    "#!pip install langchain --upgrade --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c755fbe2-fd58-4e92-b664-db4cb25ad9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain_experimental.agents.agent_toolkits import create_python_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0bd9d218-84c6-42d3-a7a7-6cf6060a769b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.25\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "55a0a127-16a7-464b-af82-2139e0b04bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Your LLM\n",
    "llm = ChatGroq(model_name=\"llama3-8b-8192\", temperature=0)\n",
    "\n",
    "# 2. Build the agent\n",
    "agent = create_python_agent(\n",
    "    llm=llm,\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5d4b8d1e-63ed-4761-b274-1f336aa8402e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to calculate the sum of 3 and 3.\n",
      "Action: Python_REPL\n",
      "Action Input: print(3 + 3)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m6\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
      "Final Answer: 6\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# 3. Run!\n",
    "print(agent.run(\"What is 3 + 3?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "10e417b1-284a-453b-b039-49e32756e644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To find the roots of the quadratic function 3*x^2 + 2*x - 1, I can use the quadratic formula. The quadratic formula is given by (-b ± √(b^2 - 4ac)) / 2a, where a, b, and c are the coefficients of the quadratic function.\n",
      "\n",
      "Action: Python_REPL\n",
      "Action Input: from sympy import symbols, Eq, solve\n",
      "x = symbols('x')\n",
      "eq = Eq(3*x**2 + 2*x - 1, 0)\n",
      "roots = solve(eq, x)\n",
      "print(roots)\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[-1, 1/3]\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: Find the roots (zeros) if the quadratic function 3 * x * * 2 + 2 * x - 1\n",
      "\n",
      "Thought: To find the roots of the quadratic function 3*x^2 + 2*x - 1, I can use the quadratic formula. The quadratic formula is given by (-b ± √(b^2 - 4ac)) / 2a, where a, b, and c are the coefficients of the quadratic function.\n",
      "\n",
      "Action: Python_REPL\n",
      "Action Input: from sympy import symbols, Eq, solve\n",
      "x = symbols('x')\n",
      "eq = Eq(3*x**2 + 2*x - 1, 0)\n",
      "roots = solve(eq, x)\n",
      "print(roots)\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[-1, 1/3]\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: Find the roots (zeros) if the quadratic function 3 * x * * 2 + 2 * x - 1\n",
      "\n",
      "Thought: To find the roots of the quadratic function 3*x^2 + 2*x - 1, I can use the quadratic formula. The quadratic formula is given by (-b ± √(b^2 - 4ac)) / 2a, where a, b, and c are the coefficients of the quadratic function.\n",
      "\n",
      "Action: Python_REPL\n",
      "Action Input: from sympy import symbols, Eq, solve\n",
      "x = symbols('x')\n",
      "eq = Eq(3*x**2 + 2*x - 1, 0)\n",
      "roots = solve(eq, x)\n",
      "print(roots)\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[-1, 1/3]\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: The roots of the quadratic function 3*x^2 + 2*x - 1 are -1 and 1/3.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The roots of the quadratic function 3*x^2 + 2*x - 1 are -1 and 1/3.'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Find the roots (zeros) if the quadratic function 3 * x * * 2 + 2 * x - 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd44938-d149-485f-ad68-7006c5ff1739",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
